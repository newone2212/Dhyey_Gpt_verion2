# MODEL_NAME = os.path.join("models","Mistral-7B-Instruct-v0.2")
 
MODEL_NAME="meta-llama/Meta-Llama-3-8B"
 
quantization_config = BitsAndBytesConfig(
    load_in_16bit=True,
    bnb_16bit_compute_dtype=torch.float16,
    bnb_16bit_quant_type="nf4",
    bnb_16bit_use_double_quant=True,
)
 
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True,token="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")
tokenizer.pad_token = tokenizer.eos_token
 
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, torch_dtype=torch.float16,
    trust_remote_code=True,
    device_map="cuda",
    quantization_config=quantization_config
)
 
generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
generation_config.max_new_tokens = 4096
#has made change here
generation_config.temperature = 0.4
generation_config.top_p = 0.5
generation_config.do_sample = True
generation_config.repetition_penalty = 1.15
 
pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=True,
    generation_config=generation_config,
    token="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"
)
llm = HuggingFacePipeline(
    pipeline=pipeline,
)
